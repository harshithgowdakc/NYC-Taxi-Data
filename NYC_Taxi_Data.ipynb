{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshithgowdakc/NYC-Taxi-Data/blob/main/NYC_Taxi_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name : NYC Taxi Trip Time Prediction**"
      ],
      "metadata": {
        "id": "rXlmlm5Q_XhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Business Context**\n",
        "\n",
        "---\n",
        "Your task is to build a model that predicts the total ride duration of taxi trips in New York City. Your primary dataset is one released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coorniates, number of passengers, and several other variables\n"
      ],
      "metadata": {
        "id": "xaIl3eKM_q82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Type - EDA**  \n",
        "\n",
        "**Contribution - Individual**   \n",
        "\n",
        "**Team Member  - Harshith Gowda**"
      ],
      "metadata": {
        "id": "FX4-7Zk20YMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -** https://github.com/harshithgowdakc/NYC-Taxi-Data.git\n",
        "\n"
      ],
      "metadata": {
        "id": "gHL_8UDk0_ZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n",
        "\n",
        "---\n",
        "Build a machine learning model to predict the duration of NYC taxi trip.\n"
      ],
      "metadata": {
        "id": "xx0_yqfm1inO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading Data and Importing Modules**"
      ],
      "metadata": {
        "id": "5gB-mmDN10SY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px-zTJsc78Ob"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import statsmodels.formula.api as sm\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "import warnings; warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzbjH4ae8HT5"
      },
      "outputs": [],
      "source": [
        "!pip install haversine\n",
        "from haversine import haversine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHAwh5Oz8Rdf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TC06Vgn8ZGZ"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AlmaBetter/Project CPG/NYC Taxi Data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Exploration**"
      ],
      "metadata": {
        "id": "VYPr-djZ2VWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Understand More About The Data**"
      ],
      "metadata": {
        "id": "lVzPuQow2Mgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbRl3Vms8gTq"
      },
      "outputs": [],
      "source": [
        "# Viewing the data of top 5 rows to look the glimps of the data\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View the data of bottom 5 rows to look the glimps of the data\n",
        "data.tail(5)"
      ],
      "metadata": {
        "id": "o1GgHCLR2op1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYyXoyVx8koJ"
      },
      "outputs": [],
      "source": [
        "#Check shape of dataset\n",
        "data.shape "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature details:**\n",
        "\n",
        "\n",
        "\n",
        "*   **id -** a unique identifier for each trip\n",
        "\n",
        "\n",
        "*   **vendor_id -** a code indicating the provider associated with the trip record\n",
        "\n",
        "\n",
        "*   **pickup_datetime -** date and time when the meter was engaged\n",
        "\n",
        "*   **dropoff_datetime -** date and time when the meter was disengaged\n",
        "\n",
        "\n",
        "\n",
        "*   **passenger_count -** the number of passengers in the vehicle (driver entered value)\n",
        "\n",
        "\n",
        "\n",
        "*   **pickup_longitude -** the longitude where the meter was engaged\n",
        "\n",
        "\n",
        "*   **pickup_latitude -** the latitude where the meter was engaged\n",
        "\n",
        "\n",
        "*   **dropoff_longitude -** the longitude where the meter was disengaged\n",
        "\n",
        "*   **dropoff_latitude -** the latitude where the meter was disengaged\n",
        "\n",
        "\n",
        "\n",
        "*   **store_and_fwd_flag -** This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip."
      ],
      "metadata": {
        "id": "mQLXPtUpAc1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are approx 1.5 million records in our dataset."
      ],
      "metadata": {
        "id": "GdgOJu3KzOfJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQY3bF3w8oB3"
      },
      "outputs": [],
      "source": [
        "#Check count of unique id's in the dataset\n",
        "print(\"There are %d unique id's in Training dataset, which is equal to the number of records\"%(data.id.nunique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMLvvL9P8tXd"
      },
      "outputs": [],
      "source": [
        "#Check for NaN values\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no NaN/NULL record in the dataset, So we dont have to impute any record."
      ],
      "metadata": {
        "id": "PTC9O3DfyNtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVbwLlQ88xNY"
      },
      "outputs": [],
      "source": [
        "#Convert timestamp to datetime format to fetch the other details as listed below\n",
        "data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\n",
        "data['dropoff_datetime'] = pd.to_datetime(data['dropoff_datetime'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFUkpc2l80tg"
      },
      "outputs": [],
      "source": [
        "#Calculate and assign new columns to the dataframe such as weekday,\n",
        "#month and pickup_hour which will help us to gain more insights from the data.\n",
        "data['weekday'] = data.pickup_datetime.dt.weekday\n",
        "data['month'] = data.pickup_datetime.dt.month\n",
        "data['weekday_num'] = data.pickup_datetime.dt.weekday\n",
        "data['pickup_hour'] = data.pickup_datetime.dt.hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kszSfqsh8_Aw"
      },
      "outputs": [],
      "source": [
        "def calc_distance(df):\n",
        "    pickup = (df['pickup_latitude'], df['pickup_longitude'])\n",
        "    drop = (df['dropoff_latitude'], df['dropoff_longitude'])\n",
        "    return haversine(pickup, drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFNtdweZ9BtO"
      },
      "outputs": [],
      "source": [
        "#Calculate distance and assign new column to the dataframe.\n",
        "data['distance'] = data.apply(lambda x: calc_distance(x), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIOh_dce9EtI"
      },
      "outputs": [],
      "source": [
        "#Calculate Speed in km/h for further insights\n",
        "data['speed'] = (data.distance/(data.trip_duration/3600))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIVZH_vf9HUR"
      },
      "outputs": [],
      "source": [
        "#Check the type of each variable\n",
        "data.dtypes.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmz8j9Im9PZ1"
      },
      "outputs": [],
      "source": [
        "#Dummify all the categorical features like \"store_and_fwd_flag, vendor_id, month, weekday_num, pickup_hour, passenger_count\" except the label i.e. \"trip_duration\"\n",
        "\n",
        "dummy = pd.get_dummies(data.store_and_fwd_flag, prefix='flag')\n",
        "dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n",
        "data = pd.concat([data,dummy], axis = 1)\n",
        "\n",
        "dummy = pd.get_dummies(data.vendor_id, prefix='vendor_id')\n",
        "dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n",
        "data = pd.concat([data,dummy], axis = 1)\n",
        "\n",
        "dummy = pd.get_dummies(data.month, prefix='month')\n",
        "dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n",
        "data = pd.concat([data,dummy], axis = 1)\n",
        "\n",
        "dummy = pd.get_dummies(data.weekday_num, prefix='weekday_num')\n",
        "dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n",
        "data = pd.concat([data,dummy], axis = 1)\n",
        "\n",
        "dummy = pd.get_dummies(data.pickup_hour, prefix='pickup_hour')\n",
        "dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n",
        "data = pd.concat([data,dummy], axis = 1)\n",
        "\n",
        "dummy = pd.get_dummies(data.passenger_count, prefix='passenger_count')\n",
        "dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n",
        "data = pd.concat([data,dummy], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "BIPB1Jiw3IT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our dataset is complete for the further analysis before we train our model with optimal variables."
      ],
      "metadata": {
        "id": "CyklpRSpyVds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Univariate Analysis**\n",
        "---\n",
        "Univariate analysis is the analysis of one variable. It's major purpose is to describe patterns in the data consisting of single variable."
      ],
      "metadata": {
        "id": "wKivvSWXyaG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passengers**\n",
        "---\n",
        "New York City Taxi Passenger Limit says:\n",
        "\n",
        "\n",
        "*   A maximum of 4 passengers can ride in traditional cabs, there are also 5 passenger cabs that look more like minivans.\n",
        "\n",
        "*   A child under 7 is allowed to sit on a passenger's lap in the rear seat in addition to the passenger limit.\n",
        "\n",
        "So, in total we can assume that maximum 6 passenger can board the new york taxi i.e. 5 adult + 1 minor"
      ],
      "metadata": {
        "id": "4VdWU0QayzUG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsszsDhx9c5v"
      },
      "outputs": [],
      "source": [
        "pd.options.display.float_format = '{:.2f}'.format #To suppres scientific notation.\n",
        "data.passenger_count.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuR70E_M9f59"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,5))\n",
        "sns.boxplot(data.passenger_count)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   There are some trips with 0 passenger count.\n",
        "\n",
        "*   Few trips consisted of even 7, 8 or 9 passengers. Clear outliers and pointers to data inconsistency\n",
        "\n",
        "*   Most of trip consist of passenger either 1 or 2."
      ],
      "metadata": {
        "id": "cZZrdBO83Pcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Idea:**\n",
        "Passenger count is a driver entered value. Since the trip is not possible without passengers. It is evident that the driver forgot to enter the value for the trips with 0 passenger count. Lets analyze the passenger count distribution further to make it consistent for further analysis\n"
      ],
      "metadata": {
        "id": "rQC2w6903dli"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9yzP-qw9j32"
      },
      "outputs": [],
      "source": [
        "data.passenger_count.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per above details. Mean median and mode are all approx equal to 1. So we would replace the 0 passenger count with 1."
      ],
      "metadata": {
        "id": "1CZ0dUF83u3g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqQxGB4eHhuP"
      },
      "outputs": [],
      "source": [
        "data['passenger_count'] = data.passenger_count.map(lambda x: 1 if x == 0 else x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, we will remove the records with passenger count > 7, 8 or 9 as they are extreme values and looks very odd to be ocupied in a taxi."
      ],
      "metadata": {
        "id": "vEUoN7Ff32ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwN639WGHjti"
      },
      "outputs": [],
      "source": [
        "data = data[data.passenger_count <= 6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A5GAr_cHltq"
      },
      "outputs": [],
      "source": [
        "data.passenger_count.value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkCyU5WUHoR9"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data.passenger_count)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is evident that most of the trips was taken by single passenger and that is inline with our day to day observations"
      ],
      "metadata": {
        "id": "f2zwEKDm4Rxn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdZn5J1QHqdl"
      },
      "source": [
        "### **Vendor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFv2TfVfHtoi"
      },
      "source": [
        "Here we analyze taxi data only for the 2 vendors which are listed as 1 and 2 in the datset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3d70MU-HwIn"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data.vendor_id)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zenBN0PQHx_F"
      },
      "source": [
        "Though both the vendors seems to have almost equal market share. But Vendor 2 is evidently more famous among the population as per the above graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUkx2nHIH0c7"
      },
      "source": [
        "### **Distance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLOnTrefH333"
      },
      "source": [
        "Let's now have a look on the distribution of the distance across the different types of rides.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TuvbiVkHyyJ"
      },
      "outputs": [],
      "source": [
        "print(data.distance.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSx82OntH71j"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,5))\n",
        "sns.boxplot(data.distance)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P2zlmEnH9d_"
      },
      "source": [
        "**Interesting find:**\n",
        "\n",
        "\n",
        "*   There some trips with over 100 km distance.\n",
        "*   Some of the trips distance value is 0 km.\n",
        "\n",
        "---\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "*   mean distance travelled is approx 3.5 kms.\n",
        "*   standard deviation of 4.3 which shows that most of the trips are limited to the range of 1-10 kms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK4qUzdgIHik"
      },
      "outputs": [],
      "source": [
        "print(\"There are {} trip records with 0 km distance\".format(data.distance[data.distance == 0 ].count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQoWqsVLIKKZ"
      },
      "outputs": [],
      "source": [
        "data[data.distance == 0 ].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpIgsoAYIP1p"
      },
      "source": [
        "**Observations**\n",
        "\n",
        "Around 6K trip record with distance equal to 0. Below are some possible explanation for such records.\n",
        "\n",
        "\n",
        "\n",
        "1.   Customer changed mind and cancelled the journey just after accepting it.\n",
        "2.   Software didn't recorded dropoff location properly due to which dropoff location is the same as the pickup location.\n",
        "3.   Issue with GPS tracker while the journey is being finished.\n",
        "4.   Driver cancelled the trip just after accepting it due to some reason. So the trip couldn't start\n",
        "5.   Or some other issue with the software itself which a technical guy can explain\n",
        "\n",
        "There is some serious inconsistencies in the data where drop off location is same as the pickup location. We can't think off imputing the distance values considering a correlation with the duration because the dropoff_location coordinates would not be inline with the distance otherwise. We will look more to it in bivariate analysis with the Trip duration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h-F4gJWIM8-"
      },
      "outputs": [],
      "source": [
        "# Group the data into bins and plot a horizontal bar chart\n",
        "data.distance.groupby(pd.cut(data.distance, np.arange(0, 100, 10))).count().plot(kind='barh', color='red')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_O1QhwUI35Y"
      },
      "source": [
        "From the above observation it is evident that most of the rides are completed between 1-10 Kms with some of the rides with distances between 10-30 kms. Other slabs bar are not visible because the number of trips are very less as compared to these slabs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sqA8VG6I6gn"
      },
      "source": [
        "### **Trip duration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g007ZZqkI4fI"
      },
      "outputs": [],
      "source": [
        "data.trip_duration.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpd5QzUUI_8q"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,5))\n",
        "sns.boxplot(data.trip_duration)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYXc7vzPJCm9"
      },
      "source": [
        "**Interesting find:**\n",
        "\n",
        "*   Some trip durations are over 100000 seconds which are clear outliers and should be removed.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "*   There are some durations with as low as 1 second. which points towards trips with 0 km distance.\n",
        "*   Major trip durations took between 10-20 mins to complete.\n",
        "*   Mean and mode are not same which shows that trip duration distribution is skewed towards right.\n",
        "\n",
        "Let's analyze more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls27cff7JaVY"
      },
      "outputs": [],
      "source": [
        "data.trip_duration.groupby(pd.cut(data.trip_duration, np.arange(1,max(data.trip_duration),3600))).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yQiD1ZvJqOC"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "*   There are some trips with more than 24 hours of travel duration i.e. 86400 seconds. Which might have occured on weekends for the outstation travels.\n",
        "*   Major chunk of trips are completed within an interval of 1 hour with some good numbers of trips duration going above 1 hour.\n",
        "\n",
        "Let's look at those trips with huge duration, these are outliers and should be removed for the data consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NWWCWMOJlAf"
      },
      "outputs": [],
      "source": [
        "data[data.trip_duration > 86400]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W24GSFuXJ5YP"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "*   These trips ran for more than 20 days, which seems unlikely by the distance travelled.\n",
        "*   All the trips are taken by vendor 1 which points us to the fact that this vendor might allows much longer trip for outstations.\n",
        "*   All these trips are either taken on Tuesday's in 1st month or Saturday's in 2nd month. There might be some relation with the weekday, pickup location, month and the passenger.\n",
        "*   But they fail our purpose of correct prediction and bring inconsistencies in the algorithm calculation.\n",
        "\n",
        "We should get rid of them for the sake of data consistency. Those are **black** swans !!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlrrEBygJ2sq"
      },
      "outputs": [],
      "source": [
        "data = data[data.trip_duration <= 86400]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q48I68VnKMdl"
      },
      "source": [
        "Let's visualize the number of trips taken in slabs of 0-10, 20-30 ... minutes respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on2_DIp0KNN3"
      },
      "outputs": [],
      "source": [
        "data.trip_duration.groupby(pd.cut(data.trip_duration, np.arange(1,7200,600))).count().plot(kind='barh')\n",
        "plt.xlabel('Trip Counts')\n",
        "plt.ylabel('Trip Duration (seconds)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRmQYQrfKR_D"
      },
      "source": [
        "We can observe that most of the trips took 0 - 30 mins to complete i.e. approx 1800 secs. Let's move ahead to next feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OPw8DoFKVIj"
      },
      "source": [
        "### **Speed**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viyttWOQKbt8"
      },
      "source": [
        "Speed is a function of distance and time. Let's visualize speed in different trips.\n",
        "\n",
        "Maximum speed limit in NYC is as follows:\n",
        "\n",
        "*   25 mph in urban area i.e. 40 km/h\n",
        "\n",
        "*   65 mph on controlled state highways i.e. approx 104 km/h\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LWORgcVKQKt"
      },
      "outputs": [],
      "source": [
        "data.speed.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfsKiK7nKqiY"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,5))\n",
        "sns.boxplot(data.speed)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFs1A-xvKszL"
      },
      "source": [
        "**Interesting find:**\n",
        "*   Many trips were done at a speed of over 200 km/h. Going SuperSonic..!!\n",
        "\n",
        "Let's remove them and focus on the trips which were done at less than 104 km/h as per the speed limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdh1nOmHKzwP"
      },
      "outputs": [],
      "source": [
        "data = data[data.speed <= 104]\n",
        "plt.figure(figsize = (20,5))\n",
        "sns.boxplot(data.speed)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykTSJCPIK4HW"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   Trips over 30 km/h are being considered as outliers but we cannot ignore them because they are well under the highest speed limit of 104 km/h on state controlled highways.\n",
        "*   Mostly trips are done at a speed range of 10-20 km/h with an average speed of around 14 km/h.\n",
        "\n",
        "Let's take a look at the speed range ditribution with the help of graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVJS7IYkLDsb"
      },
      "outputs": [],
      "source": [
        "data.speed.groupby(pd.cut(data.speed, np.arange(0,104,10))).count().plot(kind = 'barh')\n",
        "plt.xlabel('Trip count')\n",
        "plt.ylabel('Speed (Km/H)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83mIohakLFeN"
      },
      "source": [
        "It is evident from this graph what we thought off earlier i.e. most of the trips were done at a speed range of 10-20 km/H."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1662OFqELINe"
      },
      "source": [
        "### **Store_and_fwd_flag**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AnFWCmrLL0p"
      },
      "source": [
        "This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pnek--GPLGcQ"
      },
      "outputs": [],
      "source": [
        "data.flag_Y.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktDgo6FiLQ5E"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   Above result shows that only about 1% of the trip details were stored in the vehicle first before sending it to the server. This might have occured because of the following reasons:\n",
        "\n",
        "\n",
        "        1.   Outstation trips didn't had proper connection at the time when trip completes.\n",
        "\n",
        "        2.   Temporary loss of signals while the trip was about to finish\n",
        "\n",
        "        3.   Inconsistent signal reception over the trip duration.\n",
        "\n",
        "        4.   The GPS or mobile device battery was down when the trip finished.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's check further\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBAk9RfWLWun"
      },
      "outputs": [],
      "source": [
        "data.flag_Y.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLWUfKVDLZGG"
      },
      "source": [
        "Above result shows that around 8K trips had to store the flag and then report to the server when the connection was established. Let's check the respective distribution with the vendors for the offline trips."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGPr2W4TLZw4"
      },
      "outputs": [],
      "source": [
        "data.vendor_id[data.flag_Y == 1].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZRYPIM6LdmA"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   Above result shows that all the offline trips were taken by vendor 1. We already know that vendor 2 has greater market share as compared to vendor 1. So, there can be two reasons for this scenario.\n",
        "\n",
        "  1.   Either vendor 1 utilizes advance technology then vendor 2 to store and forward trip details in case of temporary signal loss.\n",
        "\n",
        "  2.   Or vendor 1 uses poor infrastructure which often suffers from the server connection instability due to which they have to store the trip info in the vehicle and send it to the server later when the server connection is back.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Let's check if there is some relation to the other metrics for these trips?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRn3e_NeLgMo"
      },
      "outputs": [],
      "source": [
        "data[data.flag_Y == 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZYxzT-gLlWX"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   Some trips are local some cover longer distance.\n",
        "*   Almost each day is listed against offline trips.\n",
        "*   Offline trips were taken almost at all hours as per the search result.\n",
        "*   There is no month which appears to be more dominant in the results.\n",
        "*   Even the trip duration covers different scales.\n",
        "\n",
        "\n",
        "---\n",
        "So all in all there doesn't seems to be any relation with either of the metric for the offline trips. Let's move ahead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dor8dGrdLqEG"
      },
      "source": [
        "### **Total trips Per Hour**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNZkbc8sLwnX"
      },
      "source": [
        "Let's take a look at the distribution of the pickups across the 24 hour time scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k26P1gQjLoEx"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data.pickup_hour)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5u3-KfZL0uR"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "*   It's inline with the general trend of taxi pickups which starts increasing from 6AM in the morning and then declines from late evening i.e. around 8 PM. There is no unusual behavior here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF-dGMU5L7yS"
      },
      "source": [
        "### **Total trips per weekday**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTqYoW0fL-gR"
      },
      "source": [
        "Let's take a look now at the distribution of taxi pickups across the week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_KVLpGFL5__"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8,6))\n",
        "sns.countplot(data.weekday_num)\n",
        "plt.xlabel(' Month ')\n",
        "plt.ylabel('Pickup counts')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3-YqVdKMDFJ"
      },
      "source": [
        "**Observation:**\n",
        "*   Here we can see an increasing trend of taxi pickups starting from Monday till Friday. The trend starts declining from saturday till monday which is normal where some office going people likes to stay at home for rest on the weekends.\n",
        "\n",
        "Let's drill down more to see the hourwise pickup pattern across the week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y3DkUgqMH7v"
      },
      "outputs": [],
      "source": [
        "n = sns.FacetGrid(data, col='weekday_num')\n",
        "n.map(plt.hist, 'pickup_hour')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD8zz_jnMS9R"
      },
      "source": [
        "**Interesting find:**\n",
        "*   Taxi pickups increased in the late night hours over the weekend possibly due to more outstation rides or for the late night leisures nearby activities.\n",
        "\n",
        "\n",
        "*   Early morning pickups i.e before 5 AM have increased over the weekend in comparison to the office hours pickups i.e. after 7 AM which have decreased due to obvious reasons.\n",
        "\n",
        "*   Taxi pickups seems to be consistent across the week at 15 Hours i.e. at 3 PM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFtKzvlKMfKW"
      },
      "source": [
        "### **Total trips per month**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI_8zp_2Mh1r"
      },
      "source": [
        "Let's take a look at the trip distribution across the months to understand if there is any diffrence in the taxi pickups in different months"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpHuGkacMcyr"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data.month)\n",
        "plt.ylabel('Trip Counts')\n",
        "plt.xlabel('Months')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxmnr9VGMm6a"
      },
      "source": [
        "Quite a balance across the months here. It could have been more equivalent if we wouldn't have removed the inconsistent records in our study of the univariate analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZZhiVfHMpmG"
      },
      "source": [
        "## **Bivariate Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0jsrfeYMufO"
      },
      "source": [
        "Bivariate analysis is used to find out if there is a relationship between two sets of values. It usually involves the variables X and Y."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Trip Duration per hour**\n",
        "---\n",
        "We need to aggregate the total trip duration to plot it agaist the month. The aggregation measure can be anything like sum, mean, median or mode for the duration. Since we already did the outlier analysis, so we can take the mean to visualize the pattern which should not result in the bias of the general trend.\n",
        "\n",
        "\n",
        "\n",
        "*   Lets take a look."
      ],
      "metadata": {
        "id": "A-fJS_5g9b5e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKeOvRaAMnya"
      },
      "outputs": [],
      "source": [
        "group1 = data.groupby('pickup_hour').trip_duration.mean()\n",
        "sns.pointplot(group1.index, group1.values)\n",
        "plt.ylabel('Trip Duration (seconds)')\n",
        "plt.xlabel('Pickup Hour')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   Average trip duration is lowest at 6 AM when there is minimal traffic on the roads.\n",
        "\n",
        "*   Average trip duration is generally highest around 3 PM during the busy streets.\n",
        "\n",
        "*   Trip duration on an average is similar during early morning hours i.e. before 6 AM & late evening hours i.e. after 6 PM."
      ],
      "metadata": {
        "id": "xxoexjvf9wWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Trip duration per weekday**\n",
        "\n",
        "---\n",
        "Let's now analyze the pattern of trip duration during the week."
      ],
      "metadata": {
        "id": "kjAPWpxv-H63"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2QekE02Mzsf"
      },
      "outputs": [],
      "source": [
        "group2 = data.groupby('weekday_num').trip_duration.mean()\n",
        "sns.pointplot(group2.index, group2.values)\n",
        "plt.ylabel('Trip Duration (seconds)')\n",
        "plt.xlabel('Weekday')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that trip duration is almost equally distributed across the week on a scale of 0-1000 minutes with minimal difference in the duration times. Also, it is observed that trip duration on thursday is longest among all days."
      ],
      "metadata": {
        "id": "24MzZTkl-S29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Trip duration per month**\n",
        "---\n",
        "Let's take a look at the trip duration pattern with respect to the different months."
      ],
      "metadata": {
        "id": "fQ578bR6-WjH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9ds7CpaM02g"
      },
      "outputs": [],
      "source": [
        "group3 = data.groupby('month').trip_duration.mean()\n",
        "sns.pointplot(group3.index, group3.values)\n",
        "plt.ylabel('Trip Duration (seconds)')\n",
        "plt.xlabel('Month')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   We can see an increasing trend in the average trip duration along with each subsequent month.\n",
        "\n",
        "\n",
        "*   The duration difference between each month is not much. It has increased gradually over a period of 6 months.\n",
        "\n",
        "*   It is lowest during february when winters starts declining.\n",
        "\n",
        "\n",
        "*   There might be some seasonal parameters like wind/rain which can be a factor of this gradual increase in trip duration over a period. Like May is generally the considered as the wettest month in NYC and which is inline with our visualization. As it generally takes longer on the roads due to traffic jams during rainy season. So natually the trip duration would increase towards April May and June."
      ],
      "metadata": {
        "id": "9elDj8_H-gUq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMpQl4n_M46l"
      },
      "source": [
        "### **Trip duration per vendor**\n",
        "\n",
        "---\n",
        "We can also look at the average difference between the trip duration for each vendor. However we do know that vendor 2 has larger share of the market. Let's visualize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjw05-gLM6fy"
      },
      "outputs": [],
      "source": [
        "group4 = data.groupby('vendor_id').trip_duration.mean()\n",
        "sns.barplot(group4.index, group4.values)\n",
        "plt.ylabel('Trip Duration (seconds)')\n",
        "plt.xlabel('Vendor')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko57hb98NCqf"
      },
      "source": [
        "Vendor 2 takes the crown. Average trip duration for vendor 2 is higher than vendor 1 by approx 200 seconds i.e. atleast 3 minutes per trip."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz7fP5kINEkb"
      },
      "source": [
        "### **Trip duration v/s Flag**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVS4L6zCNIww"
      },
      "source": [
        "Let's visualize if there is any effect of flag setting on the trip duration?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHuO32NyNA-0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (6,5))\n",
        "plot_dur = data.loc[(data.trip_duration < 10000)]\n",
        "sns.boxplot(x = \"flag_Y\", y = \"trip_duration\", data = plot_dur)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cvzc-CWNO9_"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "*   Trip durations scale is less for the trips where the flag is set i.e. the trip details are stored before sending it to the server.\n",
        "*   Trip duration outliers are also less for the trips with flag 'Y' as compared the trips with flag 'N'.\n",
        "*   Trip duration is longer for the trips where the flag is not set.\n",
        "*   Inter quartile range of trip duration is more for the trips with the flag 'Y' as compared to the trips with flag 'N' but the median value is almost equal for both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fdcf7zj3NhUS"
      },
      "source": [
        "### **Distance per hour**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZKMuIgVNlhJ"
      },
      "source": [
        "Now, let us check how the distance is distributed against different variables. We know that trip distance must be more or less proportional to the trip duration if we ignore general traffic and other stuff on the road. Let's visualize this for each hour now.\n",
        "\n",
        "Since we have already done the outlier analysis for this variable as well. We can take the mean as aggregate measure for our visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bupNT76NfJI"
      },
      "outputs": [],
      "source": [
        "group5 = data.groupby('pickup_hour').distance.mean()\n",
        "sns.pointplot(group5.index, group5.values)\n",
        "plt.ylabel('Distance (km)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPgh2P_dN4GD"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   Trip distance is highest during early morning hours which can account for some things like:\n",
        "    1.   Outstation trips taken during the weekends.\n",
        "\n",
        "    2.   Longer trips towards the city airport which is located in the outskirts of the city.\n",
        "\n",
        "\n",
        "\n",
        "*   Trip distance is fairly equal from morning till the evening varying around 3 - 3.5 kms.\n",
        "\n",
        "*   It starts increasing gradually towards the late night hours starting from evening till 5 AM and decrease steeply towards morning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqmd4x-WOdV5"
      },
      "source": [
        "### **Distance per weekday**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swQjKdC3PJqd"
      },
      "source": [
        "Let's analyze the average trip distance covered on each day of the week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRsnCKIlNxIc"
      },
      "outputs": [],
      "source": [
        "group6 = data.groupby('weekday_num').distance.mean()\n",
        "sns.pointplot(group6.index, group6.values)\n",
        "plt.ylabel('Distance (km)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCqNCrdOP1yq"
      },
      "source": [
        "So it's a fairly equal distribution with average distance metric verying around 3.5 km/h with Sunday being at the top may be due to outstation trips or night trips towards the airport."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPYl8-PbP7H9"
      },
      "source": [
        "### **Distance per month**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MncduIzrQdbk"
      },
      "source": [
        "Now we will look at the average trip distance covered per month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OjskO_YPzlS"
      },
      "outputs": [],
      "source": [
        "group7 = data.groupby('month').distance.mean()\n",
        "sns.pointplot(group7.index, group7.values)\n",
        "plt.ylabel('Distance (km)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsXcA7yvQ6b3"
      },
      "source": [
        "Here also the distibution is almost equivalent, varying mostly around 3.5 km/h with 5th month being the highest in the average distance and 2nd month being the lowest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIR7l1T-RAaS"
      },
      "source": [
        "### **Distance per vendor**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check how both the vendors have covered the average distance during the trips"
      ],
      "metadata": {
        "id": "TB1-UMHw_dLh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAP24bCqQ8UF"
      },
      "outputs": [],
      "source": [
        "group8 = data.groupby('vendor_id').distance.mean()\n",
        "sns.barplot(group8.index, group8.values)\n",
        "plt.ylabel(\"Distance km\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKDEWjqvTMiJ"
      },
      "source": [
        "This is more or less same picture with both the vendors. Nothing more to analyze in this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQm8fm-vTPDU"
      },
      "source": [
        "### **Distance v/s Flag**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Drv5vj2TTlK"
      },
      "source": [
        "Let's visualize if there is any effect of Flag setting on the distance covered in the trips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGG03dyFTVgH"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (6,6))\n",
        "plot_dist = data.loc[(data.distance < 100)]\n",
        "sns.boxplot(x = \"flag_Y\", y = \"distance\", data = plot_dist)\n",
        "plt.ylabel('Distance (km)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-id9JkVTy62"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   We can see almost similar results like the one observed in the Trip duration v/s Flag analysis.\n",
        "\n",
        "*   Only two major difference can be seen here.\n",
        "    1.   Interquartile range of distance is almost twice for Flag 'Y' trips as compared to the Flag 'N' trips\n",
        "\n",
        "    2.   Median value is much different in both the case as well.\n",
        "\n",
        "Which points us to the fact that range of distance and trip duration for the Flag 'Y' trips is much more limited and confined as compared with the flag 'N' trips and this also resulted in much less number of outliers for Flag 'Y' trips."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa-_Lz9bZ3kf"
      },
      "source": [
        "### **Distance v/s Trip duration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3wPlI8aZ7TA"
      },
      "source": [
        "Let's visualize the relationship between Distance covered and respective trip duration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG2992a3Zhys"
      },
      "outputs": [],
      "source": [
        "plt.scatter(data.trip_duration, data.distance , s=1, alpha=0.5)\n",
        "plt.ylabel('Distance')\n",
        "plt.xlabel('Trip Duration')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6DqvEapc4mF"
      },
      "source": [
        "**Interesting find:**\n",
        "\n",
        "\n",
        "*   There are lots of trips which covered negligible distance but clocked more than 20,000 seconds in terms of the Duration.\n",
        "\n",
        "*   Initially there is some proper correlation between the distance covered and the trip duration in the graph. but later on it all seems uncorrelated.\n",
        "\n",
        "*   There were few trips which covered huge distance of approx 200 kms within very less time frame, which is unlikely and should be treated as outliers.\n",
        "\n",
        "\n",
        "Let's focus on the graph area where distance is < 50 km and duration is < 1000 seconds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDH6reLD9vZQ"
      },
      "outputs": [],
      "source": [
        "dur_dist = data.loc[(data.distance < 50) & (data.trip_duration < 1000)]\n",
        "plt.scatter(dur_dist.trip_duration, dur_dist.distance , s=1, alpha=0.5)\n",
        "plt.ylabel('Distance')\n",
        "plt.xlabel('Trip Duration')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmTGcQLn-GRC"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "*   There should have been a linear relationship between the distance covered and trip duration on an average but we can see dense collection of the trips in the lower right corner which showcase many trips with the inconsistent readings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE36Uyrf-Ta-"
      },
      "source": [
        "**Idea:**\n",
        "\n",
        "We should remove those trips which covered 0 km distance but clocked more than 1 minute to make our data more consistent for predictive model. Because if the trip was cancelled after booking, than that should not have taken more than a minute time. This is our assumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvBSEVqM-rnM"
      },
      "outputs": [],
      "source": [
        "data = data[~((data.distance == 0) & (data.trip_duration >= 60))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1CEfy8U_kTh"
      },
      "source": [
        "Now, Instead of looking at each and every trip, we should approximate and try to filter those trips which covered less than 1 km distance and but clocked more than an hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVjR0XI-_xsM"
      },
      "outputs": [],
      "source": [
        "duo = data.loc[(data['distance'] <= 1) & (data['trip_duration'] >= 3600),['distance','trip_duration']].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIWeXg4P_-WA"
      },
      "outputs": [],
      "source": [
        "sns.regplot(duo.distance, duo.trip_duration)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzyE3ZSmBaT4"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   Though the straight line tries to show some linear relation between the two. But there seems to be negligible correlation between these two metric as seen from the scatter plot where it should have been a linear distribution.\n",
        "\n",
        "*   It is rarely occurs that customer keep sitting in the taxi for more than an hour and it does not travel for even 1 km.\n",
        "\n",
        "These should be removed to bring in more consistency to our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BooR5eXFvX8"
      },
      "outputs": [],
      "source": [
        "data = data[~((data['distance'] <= 1) & (data['trip_duration'] >= 3600))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLz5hdfxGU1k"
      },
      "source": [
        "### **Average speed per hour**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S9DAWE9GalT"
      },
      "source": [
        "Let's look at the average speed of NYC Taxi per hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XYDV0YfGpd_"
      },
      "outputs": [],
      "source": [
        "group9 = data.groupby('pickup_hour').speed.mean()\n",
        "sns.pointplot(group9.index, group9.values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89arz22MGl7d"
      },
      "source": [
        "Observation:\n",
        "\n",
        "*   The average trend is totally inline with the normal circumstances.\n",
        "*   Average speed tend to increase after late evening and continues to increase gradually till the late early morning hours.\n",
        "*   Average taxi speed is highest at 5 AM in the morning, then it declines steeply as the office hours approaches.\n",
        "*   Average taxi speed is more or less same during the office hours i.e. from 8 AM till 6PM in the evening."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p59XJrNEHEeB"
      },
      "source": [
        "### **Average speed per weekday**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-jp8mAWHHgm"
      },
      "source": [
        "Let's visualize that on an average what is the speed of a taxi on any given weekday."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBIOfDAmHGTi"
      },
      "outputs": [],
      "source": [
        "group10 = data.groupby('weekday_num').speed.mean()\n",
        "sns.pointplot(group10.index, group10.values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVYqcAx4Hpqb"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "\n",
        "*   Average taxi speed is higher on weekend as compared to the weekdays which is obvious when there is mostly rush of office goers and business owners.\n",
        "\n",
        "*   Even on monday the average taxi speed is shown higher which is quite surprising when it is one of the most busiest day after the weekend. There can be several possibility for such behaviour\n",
        "\n",
        "    1.   Lot of customers who come back from outstation in early hours of Monday before 6 AM to attend office on time.\n",
        "    2.   Early morning hours customers who come from the airports after vacation to attend office/business on time for the coming week.\n",
        "\n",
        "*   There could be some more reasons as well which only a local must be aware of.\n",
        "*  We also can't deny the anomalies in the dataset. which is quite cumbersome to spot in such a large dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNE05xkYIURZ"
      },
      "source": [
        "### **Passenger count per vendor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y15k9HBRIW1P"
      },
      "source": [
        "Let's try some different metric in the series i.e. passenger count. We will plot it agaist the vendor only because it will not be much helpful to plot it against hour, weekday or month like others as the passenger count should be a whole number and not a ratio.\n",
        "\n",
        "we will take mean as the aggregate measure because we already did the outlier analysis on this metric. So our results woudn't be affected by some extreme values. Also if we take median than it will return only 1 because majorty of the trips have been taken by single passenger. Let's take a look about it's distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhB9GD5UIkRd"
      },
      "outputs": [],
      "source": [
        "group9 = data.groupby('vendor_id').passenger_count.mean()\n",
        "sns.barplot(group9.index, group9.values)\n",
        "plt.ylabel('Passenger count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofVMrnpbI2IT"
      },
      "source": [
        "Clear difference between the two operators for the average passenger count in all trips. It seems that vendor 2 trips generally consist of 2 passengers as compared to the vendor 1 with 1 passenger. Let's bifurcate it further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWGzPBAVI1XC"
      },
      "outputs": [],
      "source": [
        "data.groupby('passenger_count').vendor_id.value_counts().reset_index(name='count').pivot(\"passenger_count\",\"vendor_id\",\"count\").plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo7rxRQsKBHk"
      },
      "source": [
        "**Interesting find:**\n",
        "\n",
        "\n",
        "*   It seems that most of the big cars are served by the Vendor 2 including minivans because other than passenger 1, vendor 2 has majority in serving more than 1 passenger count and that explains it greater share of the market."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAHD-_d4NLxS"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLmI67ciNPrU"
      },
      "source": [
        "After looking at the dataset from different perspectives. Let's prepare our dataset before training our model. Since our dataset do not contain very large number of dimensions. We will first try to use feature selection instead of the feature extraction technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipoqFU0FNl5t"
      },
      "source": [
        "## **Feature Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3i-cCo7Nuhd"
      },
      "source": [
        "**Intuition:**\n",
        "\n",
        "*   We will use backward elimination technique to select the best features to train our model.\n",
        "\n",
        "*   It displays some statistical metrics with there significance value.\n",
        "\n",
        "*   Like, It shows the p values for each feature as per its significance in the whole dataset.\n",
        "\n",
        "*   It also shows the adjusted R squared values to identify whether removing or selecting the feature is beneficial or not.\n",
        "\n",
        "*   For now we will only look at the P and adjusted R squared value to decide which features to keep and which needed to be removed.\n",
        "\n",
        "Let's assign the values to X & Y array from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7dDcUue7iqk"
      },
      "outputs": [],
      "source": [
        "#First chech the index of the features and label\n",
        "list(zip( range(0,len(data.columns)),data.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlXCxqkSCWtT"
      },
      "outputs": [],
      "source": [
        "Y = data.iloc[:,10].values\n",
        "X = data.iloc[:,range(15,61)].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seqp-_lNlfmd"
      },
      "source": [
        "**Question:**\n",
        "\n",
        "Why few features are not assigned to the X array like features at the index 2,3,10 were missed?\n",
        "\n",
        "Idea:\n",
        "\n",
        "*   duration variable assigned to Y because that is the dependent variable.\n",
        "\n",
        "*   features such as id, timestamp and weekday were not assigned to X array because they are of type object. And we need an array of float data type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYiRkca5onTp"
      },
      "source": [
        "Trick for backward elimination:\n",
        "General equation for multiple linear regression is like\n",
        "\n",
        "Y = a0 + a1x1 + a2x2 + ... + anxn\n",
        "\n",
        "Since, we dont have x0 in our X array so the regressor won't consider the constant value of the equation i.e. a0. So to make it count in the equation we will append the selected feature set with a contant series of 1's as a first column. To make it appear like below equation to the statsmodel.\n",
        "\n",
        "y = a0x0 + a1x1 + a2x2 + ... + anxn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwAjJ4RY9DHX"
      },
      "outputs": [],
      "source": [
        "print(\"Let's append {} rows of 1's as the first column in the X array\".format(X.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xxGJj1d9H8b"
      },
      "outputs": [],
      "source": [
        "X1 = np.append(arr = np.ones((X.shape[0],1)).astype(int), values = X, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnupnmVx9TI4"
      },
      "outputs": [],
      "source": [
        "X1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slwdQM-go5dG"
      },
      "source": [
        "There we go, our feature set is now ready for the feature selection model with 1s in the first column for a0 constant.\n",
        "\n",
        "Let's fit stats model on the X array to figure out an optimal set of features by recursively checking for the highest p value and removing the feature of that index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPc62CgQpPzL"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "Here we will take the level of significance as 0.05 i.e. 5% which means that we will reject feature from the list of array and re-run the model till p value for all the features goes below .05 to find out the optimal combination for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTH9gc3bC_V2"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ROPtNVwCt5_"
      },
      "outputs": [],
      "source": [
        "#Select all the features in X array\n",
        "X_opt = X1[:,range(0,46)]\n",
        "regressor_OLS = sm.OLS(endog = Y, exog = X_opt).fit()\n",
        "\n",
        "#Fetch p values for each feature\n",
        "p_Vals = regressor_OLS.pvalues\n",
        "\n",
        "#define significance level for accepting the feature.\n",
        "sig_Level = 0.05\n",
        "\n",
        "#Loop to iterate over features and remove the feature with p value less than the sig_level\n",
        "while max(p_Vals) > sig_Level:\n",
        "    print(\"Probability values of each feature \\n\")\n",
        "    print(p_Vals)\n",
        "    X_opt = np.delete(X_opt, np.argmax(p_Vals), axis = 1)\n",
        "    print(\"\\n\")\n",
        "    print(\"Feature at index {} is removed \\n\".format(str(np.argmax(p_Vals))))\n",
        "    print(str(X_opt.shape[1]-1) + \" dimensions remaining now... \\n\")\n",
        "    regressor_OLS = sm.OLS(endog = Y, exog = X_opt).fit()\n",
        "    p_Vals = regressor_OLS.pvalues\n",
        "    print(\"=================================================================\\n\")\n",
        "\n",
        "#Print final summary\n",
        "print(\"Final stat summary with optimal {} features\".format(str(X_opt.shape[1]-1)))\n",
        "regressor_OLS.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eyvH-7UT2f1"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "Finally we have reached the combination of optimum features with each feature having p value < 0.05."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf1ZI2pMUD7p"
      },
      "source": [
        "### **Split Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBu4bc30T-Pd"
      },
      "source": [
        "Before training our model on the dataset, we need to split the dataset into training and testing datasets. This is required to train our model on the major part of our dataset and test the accuracy of the model on the minor part. Let's split it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16uevdJMDydR"
      },
      "outputs": [],
      "source": [
        "#Split raw data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, random_state=4, test_size=0.2)\n",
        "\n",
        "#Split data from the feature selection group\n",
        "X_train_fs, X_test_fs, y_train_fs, y_test_fs = train_test_split(X_opt,Y, random_state=4, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4K1DjHjUQ2w"
      },
      "source": [
        "This will divide our dataset randomly with a ratio of 80/20 where training set consists of more than 1 million records and test dataset with more than .35 million records. Let's train our model on the training set now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRpH_b74UVus"
      },
      "source": [
        "## **Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QBADXphUZqG"
      },
      "source": [
        "We will use PCA for feature extraction i.e. Principal Component Analysis. It is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O634_k0DUllH"
      },
      "source": [
        "**Split Data:**\n",
        "\n",
        "Lets split our data first before scaling the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdXQUca0F9KM"
      },
      "outputs": [],
      "source": [
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X,Y, random_state=4, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI95r77kUvDq"
      },
      "source": [
        "**Scale Data:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEAkA6TOU0S3"
      },
      "source": [
        "It is suggested to scale the input varibles first before applying PCA to standardise the variance and avoid the bias. Lets Scale the data using StandardScaler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWmUpv8MU52s"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_pca = scaler.fit_transform(X_train_pca)\n",
        "X_test_pca = scaler.transform(X_test_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_9_lw0NVC67"
      },
      "source": [
        "**PCA application**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWbwKaJbVFKD"
      },
      "source": [
        "Let's apply PCA technique on the training features to understand how many principal components should we select for our model to capture atleast 90% variance. For that we will take help of plot and cumsum function of numpy package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-oir3iQGGlX"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA().fit(X_train_pca)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel(\"number of components\")\n",
        "plt.ylabel(\"Cumulative explained variance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhHxB7L2GJ-W"
      },
      "outputs": [],
      "source": [
        "arr = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
        "list(zip(range(1,len(arr)), arr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oiYlZ5qVUVm"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "*   Here we can see that almost 40 variables are needed for capturing atleast 99% of the variance in the training dataset. Hence we will use the same set of variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyA64SSPGOg1"
      },
      "outputs": [],
      "source": [
        "pca_10 = PCA(n_components=40)\n",
        "X_train_pca = pca_10.fit_transform(X_train_pca)\n",
        "X_test_pca = pca_10.transform(X_test_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtJFFXNzWMBa"
      },
      "source": [
        "PCA is applied on the training and the test dataset. Our input features are now ready for the regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEZl362eWQ-W"
      },
      "source": [
        "## **Correlation Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h92aufOCWTwO"
      },
      "source": [
        "Correlation analysis is a method of statistical evaluation used to study the strength of a relationship between two or more, numerically measured, continuous variables. This analysis is useful when we need to check if there are possible connections between variables. We will utilize Heatmap for our analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bwHr8E5WYhE"
      },
      "source": [
        "**Heatmap**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdJEukiMWb5b"
      },
      "source": [
        "A heatmap is a graphical representation of data that uses a system of color-coding to represent statistical relationship between different values.\n",
        "\n",
        "Let's plot the relationship between the features of the **Feature selection** group first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQZankZAWKT9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "corr = pd.DataFrame(X_train_fs[:,1:]).corr()\n",
        "corr.index = pd.DataFrame(X_train_fs[:,1:]).columns\n",
        "sns.heatmap(corr, cmap='RdYlGn', vmin=-1, vmax=1, square=True)\n",
        "plt.title(\"Correlation Heatmap\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGMjqxGiWjCU"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   Some combinations of features shows slight correlation but not above 0.5.\n",
        "\n",
        "\n",
        "*   Some features are infact negatively correlated.\n",
        "\n",
        "*   But most of the features shows no correlation. Which is a good thing.\n",
        "\n",
        "\n",
        "All in all, we have a very good pack of attributes to train our model. Let's move ahead.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fymh64bva7A_"
      },
      "source": [
        "Let's now plot the relationship between the features of the **Feature extraction** group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z5_bp8Ta9iG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "corr = pd.DataFrame(X_train_pca).corr()\n",
        "corr.index = pd.DataFrame(X_train_pca).columns\n",
        "sns.heatmap(corr, cmap='RdYlGn', vmin=-1, vmax=1, square=True)\n",
        "plt.title(\"Correlation Heatmap\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvCwhpgKbFEM"
      },
      "source": [
        "**Observations:**\n",
        "*   All of the features shows **NO** correlation at all. Because feature extraction removes all collinearity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRwVgfG5bMCM"
      },
      "source": [
        "Let's move on to the Model now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7liGU31AGdb5"
      },
      "source": [
        "### **Model**\n",
        "\n",
        "---\n",
        "We need a model to train on our dataset to serve our purpose of prediciting the NYC taxi trip duration given the other features as training and test set. Since our dependent variable contains continous values so we will use regression technique to predict our output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7-CGn1JGmNm"
      },
      "source": [
        "### **Multiple Linear Regression**\n",
        "\n",
        "---\n",
        "It is used to explain the relationship between one continuous dependent variable and two or more independent variables. Let's proceed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ciGa6oGuMu"
      },
      "source": [
        "### **Model training**\n",
        "\n",
        "---\n",
        "We will first try with the default instantiation of the regressor object without using any generalization parameter. We will also not perform any scaling of the features because linear regression model takes care of that inherently. This is a plus point to use Linear regression model. It is quite fast to train even on very large datasets. So considering the size of our dataset this seems to be the correct approach as of now. Let's see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bElzHKeXGsSg"
      },
      "outputs": [],
      "source": [
        "#Linear regressor for the raw data\n",
        "regressor = LinearRegression() \n",
        "regressor.fit(X_train,y_train) \n",
        "\n",
        "#Linear regressor for the Feature selection group\n",
        "regressor1 = LinearRegression() \n",
        "regressor1.fit(X_train_fs,y_train_fs) \n",
        "\n",
        "#Linear regressor for the Feature extraction group\n",
        "regressor2 = LinearRegression() \n",
        "regressor2.fit(X_train_pca,y_train_pca) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sI-or-AG3Tv"
      },
      "source": [
        "**Interesting find:**\n",
        "\n",
        "\n",
        "*   It took approx 1 second to train the model on dataset of more than 1 million records.\n",
        "\n",
        "*   It is evident that Linear regression model is extremely fast to train on the high dimension datasets consisting of even millions of records.\n",
        "\n",
        "*   Linear regression object for the feature extraction group took less time to train on the input features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXNGaKO4JMSq"
      },
      "source": [
        "### **Model prediction**\n",
        "\n",
        "---\n",
        "So now, our model has been fitted to the training set. It's time to predict the dependent variable. Let's do that now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr1TI5KUJVnw"
      },
      "outputs": [],
      "source": [
        "#Predict from the test features of raw data\n",
        "y_pred = regressor.predict(X_test) \n",
        "\n",
        "#Predict from the test features of Feature Selection group\n",
        "y_pred = regressor1.predict(X_test_fs) \n",
        "\n",
        "#Predict from the test features of Feature Extraction group\n",
        "y_pred_pca = regressor2.predict(X_test_pca) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgfJ8IR4JXwm"
      },
      "source": [
        "### **Model evaluation**\n",
        "---\n",
        "We will evaluate our model's accuracy through two suggested metrics for the regression models. i.e. RMSE and variance score. Where RMSE of 0 and variance of 1 is considered as the best score for a prediction model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51y7r-HvJeBJ"
      },
      "outputs": [],
      "source": [
        "#Evaluate the regressor on the raw data\n",
        "print('RMSE score for the Multiple LR raw is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test,y_pred))))\n",
        "print('Variance score for the Multiple LR raw is : %.2f' % regressor.score(X_test, y_test))\n",
        "print(\"\\n\")\n",
        "\n",
        "#Evaluate the regressor on the Feature selection group\n",
        "print('RMSE score for the Multiple LR FS is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred))))\n",
        "print('Variance score for the Multiple LR FS is : %.2f' % regressor1.score(X_test_fs, y_test_fs))\n",
        "print(\"\\n\")\n",
        "\n",
        "#Evaluate the regressor on the Feature extraction group\n",
        "print('RMSE score for the Multiple LR PCA is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_pca,y_pred_pca))))\n",
        "print('Variance score for the Multiple LR PCA is : %.2f' % regressor2.score(X_test_pca, y_test_pca))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0cRx3SvJnDV"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   Very poor Root mean squared value.\n",
        "*   And the low variance score which is also bad.\n",
        "*   Both the models i.e. from the feature selection and the feature extraction group resulted quite bad in prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzdqKhJ4J20F"
      },
      "source": [
        "\n",
        "**Let's find out the reason of this behaviour:-**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of_rHJEFKCnA"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twFks2wEKFRr"
      },
      "outputs": [],
      "source": [
        "#Find linear correlation of each feature with the target variable\n",
        "from scipy.stats import pearsonr\n",
        "df1 = pd.DataFrame(np.concatenate((X_train,y_train.reshape(len(y_train),1)),axis=1))\n",
        "df1.columns = df1.columns.astype(str)\n",
        "\n",
        "features = df1.iloc[:,:46].columns.tolist()\n",
        "target = df1.iloc[:,46].name\n",
        "\n",
        "correlations = {}\n",
        "for f in features:\n",
        "    data_temp = df1[[f,target]]\n",
        "    x1 = data_temp[f].values\n",
        "    x2 = data_temp[target].values\n",
        "    key = f + ' vs ' + target\n",
        "    correlations[key] = pearsonr(x1,x2)[0]\n",
        "    \n",
        "data_correlations = pd.DataFrame(correlations, index=['Value']).T\n",
        "data_correlations.loc[data_correlations['Value'].abs().sort_values(ascending=False).index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaHGm_K5KQRl"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "We can see that none of the feature is linearly correlated with the target variable \"**46**\". That is why it is not a good model for the prediction of the trip duration. So let's move ahead and try the **random forest regressor**. We are not using decision tree regressor because the random forest will anyways consist of almost all its properties. Also, we will not use SVR because it takes too much time to train on this huge dataset even with the default settings. It seems to be not good with high dimensional dataset as well as for the huge instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nk2h-gf7B3f"
      },
      "source": [
        "### **Random Forest Regressor**\n",
        "---\n",
        "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAXt9XdM7Mjo"
      },
      "source": [
        "**Model training**\n",
        "\n",
        "---\n",
        "Now we will train the model on the filtered features. Our data has already been split so we will not split the data further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucnh5gNM7WIg"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "We used **GridSearch** to tune the **hyperparameters** of random forest regressor to get the best possible test score. We tried various combination of the allowed hyper params values. But any kind of combination could not produce significantly better results than the default settings. There can be many reasons for that and it totally depends on the type of data we have in hand. Therefore we will not show tuned regressor results here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
        "\n",
        "# Reduce the size of the training set and use a smaller number of trees\n",
        "n_trees = 50\n",
        "max_samples = 0.1\n",
        "X_train_small = X_train[:int(max_samples * len(X_train)), :]\n",
        "y_train_small = y_train[:int(max_samples * len(y_train))]\n",
        "regressor_rf = RandomForestRegressor(n_estimators=n_trees, n_jobs=-1)\n",
        "\n",
        "# Train the random forest regressor on the reduced training data\n",
        "regressor_rf.fit(X_train_small, y_train_small)\n",
        "\n",
        "# Evaluate the random forest regressor on the test set\n",
        "y_pred = regressor_rf.predict(X_test)\n",
        "mse = np.mean((y_test - y_pred) ** 2)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "id": "YvykutlhPLTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9999 = dig"
      ],
      "metadata": {
        "id": "-WbM2m-SPc-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1zcsbe87AfW"
      },
      "outputs": [],
      "source": [
        "#instantiate the object for the Random Forest Regressor with default params from raw data\n",
        "regressor_rfraw = RandomForestRegressor(n_jobs=-1)\n",
        "\n",
        "#instantiate the object for the Random Forest Regressor with default params for Feature Selection Group\n",
        "regressor_rf = RandomForestRegressor(n_jobs=-1)\n",
        "\n",
        "# #instantiate the object for the Random Forest Regressor with tuned hyper parameters for Feature Selection Group\n",
        "# regressor_rf1 = RandomForestRegressor(n_estimators = 26,\n",
        "#                                      max_depth = 22,\n",
        "#                                      min_samples_split = 9,\n",
        "#                                      n_jobs=-1)\n",
        "\n",
        "#instantiate the object for the Random Forest Regressor for Feature Extraction Group\n",
        "regressor_rf2 = RandomForestRegressor(n_jobs=-1)\n",
        "\n",
        "\n",
        "#Train the object with default params for raw data\n",
        "regressor_rfraw.fit(X_train,y_train)\n",
        "\n",
        "#Train the object with default params for Feature Selection Group\n",
        "regressor_rf.fit(X_train_fs,y_train_fs)\n",
        "\n",
        "# #Train the object with tuned params for Feature Selection Group\n",
        "# regressor_rf1.fit(X_train_fs,y_train_fs)\n",
        "\n",
        "# #Train the object with default params for Feature Extraction Group\n",
        "regressor_rf2.fit(X_train_pca,y_train_pca)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42i5k15nDY7s"
      },
      "source": [
        "**Model prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZVHYhzVDdkZ"
      },
      "outputs": [],
      "source": [
        "#Predict the output with object of default params for Feature Selection Group\n",
        "y_pred_rfraw = regressor_rfraw.predict(X_test)\n",
        "\n",
        "#Predict the output with object of default params for Feature Selection Group\n",
        "y_pred_rf = regressor_rf.predict(X_test_fs)\n",
        "\n",
        "# #Predict the output with object of hyper tuned params for Feature Selection Group\n",
        "# y_pred_rf1 = regressor_rf1.predict(X_test_fs)\n",
        "\n",
        "#Predict the output with object of PCA params for Feature Extraction Group\n",
        "y_pred_rfpca = regressor_rf2.predict(X_test_pca)\n",
        "\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KZOXaq6DitA"
      },
      "source": [
        "**Model evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdxnK6osDf7D"
      },
      "outputs": [],
      "source": [
        "#Evaluate the model with default params for raw data\n",
        "print('RMSE score for the RF regressor raw is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test,y_pred_rfraw))))\n",
        "print('RMSLE score for the RF regressor raw is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test,y_pred_rfraw))))\n",
        "print('Variance score for the RF regressor raw is : %.2f' % regressor_rfraw.score(X_test, y_test))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "#Evaluate the model with default params for Feature Selection Group\n",
        "print('RMSE score for the RF regressor is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred_rf))))\n",
        "print('RMSLE score for the RF regressor is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test_fs,y_pred_rf))))\n",
        "print('Variance score for the RF regressor is : %.2f' % regressor_rf.score(X_test_fs, y_test_fs))\n",
        "\n",
        "# print(\"\\n\")\n",
        "\n",
        "# #Evaluate the model with tuned params for Feature Selection Group\n",
        "# print('RMSE score for the RF regressor1 is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred_rf1))))\n",
        "# print('RMSLE score for the RF regressor1 is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test_fs,y_pred_rf1))))\n",
        "# print('Variance score for the RF regressor1 is : %.2f' % regressor_rf1.score(X_test_fs, y_test_fs))\n",
        "\n",
        "print(\"\\n\")\n",
        "#Evaluate the model with PCA params  for Feature Extraction Group\n",
        "print('RMSE score for the RF regressor2 is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_pca, y_pred_rfpca))))\n",
        "print('Variance score for the RF regressor2 is : %.2f' % regressor_rf2.score(X_test_pca, y_test_pca))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interesting find**\n",
        "\n",
        "\n",
        "*   There is approx 200% improvement on the RMSE score for the Random forest regressor over the Linear regressor of the feature selection group.\n",
        "\n",
        "\n",
        "*   Even the variance score is approx 1 which is a good score.\n",
        "\n",
        "*   RMSE score for the RF regressor of feature extraction group is still very bad along with the variance score.\n",
        "\n",
        "*   RMSE score for the feature selection group is more or less same as the raw data score. Sometimes the RMSE score for the raw data is better and vice versa. It fluctuates on every iteration and this is quite weird!\n",
        "\n",
        "---\n",
        "\n",
        "Let's see if we can improve this further with the most sought after algorigthm i.e. XGBoost!!"
      ],
      "metadata": {
        "id": "87qVR4Zy0QsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoost Regressor**\n",
        "\n",
        "---\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) is an optimized distributed gradient boosting library. It uses gradient boosting (GBM) framework at core. It belongs to a family of boosting algorithms that convert weak learners into strong learners. A weak learner is one which is slightly better than random guessing.\n",
        "\n",
        "'Boosting' here is a sequential process; i.e., trees are grown using the information from a previously grown tree one after the other. This process slowly learns from data and tries to improve its prediction in the subsequent iterations."
      ],
      "metadata": {
        "id": "iV-Fgbm-0sFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model training**\n",
        "\n",
        "---\n",
        "We will train the model on the filtered features. Our data has already been split so we will not split the data further."
      ],
      "metadata": {
        "id": "jDQpEiy11ABr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "We used GridSearch to tune the hyperparameters of XGBoost regressor to get the best possible test score. We will compare results from the default regressor and the tuned regressor."
      ],
      "metadata": {
        "id": "HwjO1woq1IW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate the object for the XGBoost Regressor with default params for raw data\n",
        "regressor_xgbraw = XGBRegressor(n_jobs=-1)\n",
        "\n",
        "#instantiate the object for the XGBoost Regressor with default params for Feature Selection Group\n",
        "regressor_xgb = XGBRegressor(n_jobs=-1)\n",
        "\n",
        "#instantiate the object for the XGBoost Regressor with tuned hyper parameters for Feature Selection Group\n",
        "regressor_xgb1 = XGBRegressor(n_estimators=300,\n",
        "                            learning_rate=0.08,\n",
        "                            gamma=0,\n",
        "                            subsample=0.75,\n",
        "                            colsample_bytree=1,\n",
        "                            max_depth=7,\n",
        "                            min_child_weight=4,\n",
        "                            silent=1,\n",
        "                           n_jobs=-1)\n",
        "\n",
        "#instantiate the object for the XGBoost Regressor for Feature Extraction Group\n",
        "regressor_xgb2 = XGBRegressor(n_jobs=-1)\n",
        "\n",
        "\n",
        "#Train the object with default params for raw data\n",
        "regressor_xgbraw.fit(X_train,y_train)\n",
        "\n",
        "#Train the object with default params for Feature Selection Group\n",
        "regressor_xgb.fit(X_train_fs,y_train_fs)\n",
        "\n",
        "#Train the object with tuned params for Feature Selection Group\n",
        "regressor_xgb1.fit(X_train_fs,y_train_fs)\n",
        "\n",
        "#Train the object with default params for Feature Extraction Group\n",
        "regressor_xgb2.fit(X_train_pca,y_train_pca)\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "YEqpXRax0qI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model prediction**"
      ],
      "metadata": {
        "id": "edmkxvT-1dNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict the output with object of default params for raw data\n",
        "y_pred_xgbraw = regressor_xgbraw.predict(X_test)\n",
        "\n",
        "#Predict the output with object of default params for Feature Selection Group\n",
        "y_pred_xgb = regressor_xgb.predict(X_test_fs)\n",
        "\n",
        "#Predict the output with object of hyper tuned params for Feature Selection Group\n",
        "y_pred_xgb1 = regressor_xgb1.predict(X_test_fs)\n",
        "\n",
        "#Predict the output with object of PCA params for Feature Extraction Group\n",
        "y_pred_xgb_pca = regressor_xgb2.predict(X_test_pca)\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "v-TohnKf1gRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "59OAONmK1iBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model with default params for raw data\n",
        "print('RMSE score for the XGBoost regressor raw is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test,y_pred_xgbraw))))\n",
        "# print('RMSLE score for the XGBoost regressor is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test,y_pred_xgb))))\n",
        "print('Variance score for the XGBoost regressor raw is : %.2f' % regressor_xgbraw.score(X_test, y_test))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "#Evaluate the model with default params for Feature Selection Group\n",
        "print('RMSE score for the XGBoost regressor is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred_xgb))))\n",
        "# print('RMSLE score for the XGBoost regressor is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test,y_pred_xgb))))\n",
        "print('Variance score for the XGBoost regressor is : %.2f' % regressor_xgb.score(X_test_fs, y_test_fs))\n",
        "\n",
        "print(\"\\n\")\n",
        "#Evaluate the model with Tuned params for Feature Selection Group\n",
        "print('RMSE score for the XGBoost regressor1 is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred_xgb1))))\n",
        "# print('RMSLE score for the XGBoost regressor1 is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test_fs,y_pred_xgb1))))\n",
        "print('Variance score for the XGBoost regressor1 is : %.2f' % regressor_xgb1.score(X_test_fs,y_test_fs))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "#Evaluate the model with PCA params  for Feature Extraction Group\n",
        "print('RMSE score for the XGBoost regressor2 is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_pca, y_pred_xgb_pca))))\n",
        "print('Variance score for the XGBoost regressor2 is : %.2f' % regressor_xgb2.score(X_test_pca, y_test_pca))"
      ],
      "metadata": {
        "id": "MZGhyiI71jfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "*   There is a significant improvement in the RMSE score for the tuned XGBoost regressor over the Random forest regressor when trained on the feature selection group.\n",
        "\n",
        "\n",
        "*   But the performance of the default XGBoost regressor is quite worse than the default RF regressor on the same data.\n",
        "\n",
        "*   Also, the RMSE score on the raw data and feature selected data are same, which disproves the theory that it is always better to select the relevant features which are statistically important. As the data behaves differently in different models.\n",
        "\n",
        "*   Not to mention the fact that RMSE score for the XGBoost regressor of the feature extraction group is still bad along with the variance score.\n"
      ],
      "metadata": {
        "id": "j-t1cL8o1tc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparing test results for the XGBoost and RF regressor\n",
        "print(\"Total sum of difference between the actual and the predicted values for the RF regressor is : %d\"%np.abs(np.sum(np.subtract(y_test,y_pred_rf))))\n",
        "print(\"Total sum of difference between the actual and the predicted values for the tuned XGB regressor is : %d\"%np.abs(np.sum(np.subtract(y_test,y_pred_xgb1))))"
      ],
      "metadata": {
        "id": "mQHz-0Dj2Dxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General inference\n",
        "\n",
        "*   XGBoost proved to be much more efficient in predicting the output. But it takes much more time to train it over the large dataset wih more complexity as compared to the RF and Linear regression model but less time then the SVR.\n",
        "\n",
        "\n",
        "*   It didn't helped us much to generalize the model by tuning hyper parameters for the RF model as there is not much difference in the RMSE scores of the default model and the tuned model of the feature selection group infact both varies on every iteration and sometimes the tuned model gives poor results than the default model. Though we tried many possible alterations with GSCV but the tuning could not achieve a significant improvement over the default model which also depends on the contents of the dataset.\n",
        "\n",
        "\n",
        "*   Contrast to the RF regressor, XGBoost regressor prediction results were consistent on every iteration i.e. for each param configuration the results were the same.\n",
        "\n",
        "*   Feature extraction didn't helped in anyway to improve the RMSE score with any of the regressor models. This shows us that the feature extraction is somewhat not a good technique to preprocess the data before feeding it into the regressor models for the continous target value prediction. Whereas it also depends on the type and features of data that how it behaves with the model.\n"
      ],
      "metadata": {
        "id": "ZuAGdulW2Fl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Learning curve**\n",
        "---\n",
        "Learning curves constitute a great tool to diagnose bias and variance in any supervised learning algorithm. It shows how error changes as the training set size increases. We'll use the learning_curve() function from the scikit-learn library to generate a learning curve for the regression model. There's no need to put aside a validation set because learning_curve() will take care of that and that's why we will plot the learning curve over whole dataset."
      ],
      "metadata": {
        "id": "AqdwCy29Mbf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a function to plot learning curve.\n",
        "def learning_curves(estimator, title, features, target, train_sizes, cv, n_jobs=-1):\n",
        "    plt.figure(figsize = (14,5))\n",
        "    train_sizes, train_scores, validation_scores = learning_curve(estimator, features, target, train_sizes = train_sizes, cv = cv, scoring = 'neg_mean_squared_error',  n_jobs=n_jobs)\n",
        "    train_scores_mean = -train_scores.mean(axis = 1)\n",
        "    validation_scores_mean = -validation_scores.mean(axis = 1)\n",
        "    \n",
        "    plt.grid()\n",
        "    \n",
        "    plt.plot(train_sizes, train_scores_mean,'o-', color=\"r\", label = 'Training error')\n",
        "    plt.plot(train_sizes, validation_scores_mean,'o-', color=\"g\", label = 'Validation error')\n",
        "\n",
        "    plt.ylabel('MSE', fontsize = 14)\n",
        "    plt.xlabel('Training set size', fontsize = 14)\n",
        "    \n",
        "    title = 'Learning curves for a ' + title + ' model'\n",
        "    plt.title(title, fontsize = 18, loc='left')\n",
        "    \n",
        "    plt.legend(loc=\"best\")\n",
        "    \n",
        "    return plt\n",
        "\n",
        "# score curves, each time with 20% data randomly selected as a validation set.\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=4)\n",
        "\n",
        "# Plot learning curve for the RF Regressor\n",
        "title = \"Random Forest Regressor\"\n",
        "\n",
        "# Call learning curve with all dataset i.e. traininig and test combined because CV will take of data split.\n",
        "learning_curves(regressor_rf, title, X_opt,Y, train_sizes=np.linspace(.1, 1.0, 5), cv=cv, n_jobs=-1)\n",
        "\n",
        "#Plot learning curve for the XGBoost Regressor\n",
        "title = \"XGBoost Regressor\"\n",
        "\n",
        "# Call learning curve on less number of estimators than the tuned estimator because it took too much time for the compilation.\n",
        "learning_curves(XGBRegressor(n_estimators=111,\n",
        "                            learning_rate=0.08,\n",
        "                            gamma=0,\n",
        "                            subsample=0.75,\n",
        "                            colsample_bytree=1,\n",
        "                            max_depth=7,\n",
        "                            min_child_weight=4,\n",
        "                            silent=1), title, X_opt,Y, train_sizes=np.linspace(.1, 1.0, 5), cv=cv, n_jobs=-1)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oyHlBOR5MkjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "\n",
        "*   We can observe that both the models shows somewhat similar learning rate but with visible differences in error rates.\n",
        "\n",
        "\n",
        "*   RF training curve initially starts high but later on improves as the training size increases and then seems to plateaud by the end.\n",
        "\n",
        "\n",
        "*   XGBoost training curve on the other hand starts quite low and further improves with the increase in the training size and it too plateau towards the end.\n",
        "\n",
        "*   Validation curve seems to show similar trend in both the models i.e. starts very high but improves with the training size with some differences in error rate i.e. XGBoost curve learning is quite fast and more accurate as compared to the RF one.\n",
        "\n",
        "\n",
        "\n",
        "*   Both the models seems to suffer from high variance since the training curve error is very less in both the models.\n",
        "\n",
        "\n",
        "\n",
        "*   The large gap at the end also indicates that the model suffers from quite a low bias i.e. overfitting the training data.\n",
        "\n",
        "\n",
        "*   Also, both the model's still has potential to decrease and converge towards the training curve by the end."
      ],
      "metadata": {
        "id": "C8hNKJTpMuNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **At this point, here are a few things we could do to improve our model:**\n",
        "\n",
        "\n",
        "1.   Add more training instances to improve validation curve in the XGBoost model.\n",
        "\n",
        "2.   Increase the regularization for the learning algorithm. This should decrease the variance and increase the bias towards the validation curve.\n",
        "\n",
        "3.   Reduce the numbers of features in the training data that we currently use. The algorithm will still fit the training data very well, but due to the decreased number of features, it will build less complex models. This should increase the bias and decrease the variance."
      ],
      "metadata": {
        "id": "9rDYSStZh4xt"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKhwfT/yNPi7f9UxNc8wov",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}